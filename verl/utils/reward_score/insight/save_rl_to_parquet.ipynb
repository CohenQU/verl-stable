{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['joint_prompt', 'paper1_prompt', 'paper2_prompt', 'no_context_prompt', 'abstracts', 'forum_id_1', 'forum_id_2', 'pair_id'],\n",
       "        num_rows: 99897\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['joint_prompt', 'paper1_prompt', 'paper2_prompt', 'no_context_prompt', 'abstracts', 'forum_id_1', 'forum_id_2', 'pair_id'],\n",
       "        num_rows: 100\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import datasets\n",
    "\n",
    "ds = datasets.load_dataset('joyheyueya/250428_abstract_pair', split='train')\n",
    "ds = ds.train_test_split(test_size=0.001, seed=42)\n",
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'joint_prompt': 'Your task is to identify and elaborate on an insight that only becomes apparent by combining information from both documents together—i.e., an insight that has high relevance when treating the documents jointly but low relevance if you were to consider each document alone. Write the insight as a standalone statement that does not reference the original texts or use terms like \"paper\" or \"document\". The insight should involve indirect connections or intermediate reasoning steps, such as linking cause and effect through a shared variable or mechanism. Here are some examples: Let A = magnesium — either dietary or internal to the body. Implicitly refers, inmost cases, to quantity of magnesium. Let M = migraine headache. Let L = one or more intermediate physiological links.  -> means \\'can or might influence\\'. == means \\'equivalent in action\\' or \\'equivalent in mechanism\\' depending on whether it connects two drugs or two diseases. Paper 1:\\nStress and Type A behavior are associated with migraine.\\nPaper 2:\\nStress and Type A behavior lead to body loss of magnesium.\\nFrom these, we infer: Let B = Stress and Type A behavior. Let A = Magnesium levels. Let M = Migraines. From paper 1: B -> M. From Paper 2: B -> A. This suggests A may mediate the effect of B on M, i.e., A -> M. <insight>Low magnesium levels may be an intermediate link through which stress and Type A behavior contribute to migraines.</insight>\\nPaper 1:\\nExcessive vascular tone and reactivity may increase susceptibility to migraine.\\nPaper 2:\\nMagnesium can reduce vascular tone and reactivity.\\nWe define: A = Magnesium. L = Vascular tone and reactivity. M = Migraine. From these: (a) suggests L -> M. (b) suggests A -> L. Therefore, combining the two leads to: A -> M. <insight>Magnesium may help reduce migraine susceptibility by lowering vascular tone and reactivity.</insight>\\nPaper 1:\\nCalcium channel blockers have been used successfully in preventing migraine attacks.\\nPaper 2:\\nMagnesium is a natural calcium channel blocker.\\nWe define: A = Magnesium. B = Calcium channel blockers. M = Migraine. From these:(a) suggests B -> M (i.e., B prevents M). (b) suggests A == B (A is equivalent to B in mechanism). Therefore: A -> M. <insight>Magnesium may help prevent migraine attacks because it acts as a natural calcium channel blocker, similar to drugs already known to be effective.</insight>\\nPaper 1:\\nCurrent benchmarks like ``$\\\\textit{Needle-in-a-Haystack}$\\'\\' ($\\\\textit{NIAH}$), $\\\\textit{Ruler}$, and $\\\\textit{Needlebench}$ focus on models\\' ability to understand long-context input sequences but fail to capture a critical dimension: the generation of high-quality long-form text. Applications such as design proposals, technical documentation, and creative writing rely on coherent, instruction-following outputs over extended sequences—a challenge that existing benchmarks do not adequately address. To fill this gap, we introduce $\\\\textit{LongGenBench}$, a novel benchmark designed to rigorously evaluate large language models\\' (LLMs) ability to generate long text while adhering to complex instructions. Through tasks requiring specific events or constraints within generated text, $\\\\textit{LongGenBench}$ evaluates model performance across four distinct scenarios, three instruction types, and two generation-lengths (16K and 32K tokens). Our evaluation of ten state-of-the-art LLMs reveals that, despite strong results on $\\\\textit{Ruler}$, all models struggled with long text generation on $\\\\textit{LongGenBench}$, particularly as text length increased. This suggests that current LLMs are not yet equipped to meet the demands of real-world, long-form text generation. We open-source $\\\\textit{LongGenBench}$ to promote comprehensive evaluation and improvement in this critical area, with code and data available at ${anonymousurl}$.\\nPaper 2:\\nLarge Multimodal Models (LMMs) have made significant strides in visual question-answering for single images. Recent advancements like long-context LMMs have allowed them to ingest larger, or even multiple, images. However, the ability to process a large number of visual tokens does not guarantee effective retrieval and reasoning for multi-image question answering (MIQA), especially in real-world applications like photo album searches or satellite imagery analysis. In this work, we first assess the limitations of current benchmarks for long-context LMMs. We address these limitations by introducing a new vision-centric, long-context benchmark, \"Visual Haystacks (VHs)\". We comprehensively evaluate both open-source and proprietary models on VHs, and demonstrate that these models struggle when reasoning across potentially unrelated images, perform poorly on cross-image reasoning, as well as exhibit biases based on the placement of key information within the context window. Towards a solution, we introduce MIRAGE (Multi-Image Retrieval Augmented Generation), an open-source, lightweight visual-RAG framework that processes up to 10k images on a single 40G A100 GPU—far surpassing the 1k-image limit of contemporary models. MIRAGE demonstrates up to 13% performance improvement over existing open-source LMMs on VHs, sets a new state-of-the-art on the RetVQA multi-image QA benchmark, and achieves competitive performance on single-image QA with state-of-the-art LMMs. Our dataset, model, and code are available at: https://visual-haystacks.github.io.\\nPut the insight between <insight> and </insight> tagsLet\\'s think step by step. ',\n",
       " 'paper1_prompt': 'Paper:\\nCurrent benchmarks like ``$\\\\textit{Needle-in-a-Haystack}$\\'\\' ($\\\\textit{NIAH}$), $\\\\textit{Ruler}$, and $\\\\textit{Needlebench}$ focus on models\\' ability to understand long-context input sequences but fail to capture a critical dimension: the generation of high-quality long-form text. Applications such as design proposals, technical documentation, and creative writing rely on coherent, instruction-following outputs over extended sequences—a challenge that existing benchmarks do not adequately address. To fill this gap, we introduce $\\\\textit{LongGenBench}$, a novel benchmark designed to rigorously evaluate large language models\\' (LLMs) ability to generate long text while adhering to complex instructions. Through tasks requiring specific events or constraints within generated text, $\\\\textit{LongGenBench}$ evaluates model performance across four distinct scenarios, three instruction types, and two generation-lengths (16K and 32K tokens). Our evaluation of ten state-of-the-art LLMs reveals that, despite strong results on $\\\\textit{Ruler}$, all models struggled with long text generation on $\\\\textit{LongGenBench}$, particularly as text length increased. This suggests that current LLMs are not yet equipped to meet the demands of real-world, long-form text generation. We open-source $\\\\textit{LongGenBench}$ to promote comprehensive evaluation and improvement in this critical area, with code and data available at ${anonymousurl}$.\\nYour task is to identify and elaborate on an insight from the paper. The insight should be self-contained. Write it in a way that doesn’t require referencing where it came from. Do not mention the word \"paper\" or \"document\". Let\\'s think step by step. ',\n",
       " 'paper2_prompt': 'Paper:\\nLarge Multimodal Models (LMMs) have made significant strides in visual question-answering for single images. Recent advancements like long-context LMMs have allowed them to ingest larger, or even multiple, images. However, the ability to process a large number of visual tokens does not guarantee effective retrieval and reasoning for multi-image question answering (MIQA), especially in real-world applications like photo album searches or satellite imagery analysis. In this work, we first assess the limitations of current benchmarks for long-context LMMs. We address these limitations by introducing a new vision-centric, long-context benchmark, \"Visual Haystacks (VHs)\". We comprehensively evaluate both open-source and proprietary models on VHs, and demonstrate that these models struggle when reasoning across potentially unrelated images, perform poorly on cross-image reasoning, as well as exhibit biases based on the placement of key information within the context window. Towards a solution, we introduce MIRAGE (Multi-Image Retrieval Augmented Generation), an open-source, lightweight visual-RAG framework that processes up to 10k images on a single 40G A100 GPU—far surpassing the 1k-image limit of contemporary models. MIRAGE demonstrates up to 13% performance improvement over existing open-source LMMs on VHs, sets a new state-of-the-art on the RetVQA multi-image QA benchmark, and achieves competitive performance on single-image QA with state-of-the-art LMMs. Our dataset, model, and code are available at: https://visual-haystacks.github.io.\\nYour task is to identify and elaborate on an insight from the paper. The insight should be self-contained. Write it in a way that doesn’t require referencing where it came from. Do not mention the word \"paper\" or \"document\". Let\\'s think step by step. ',\n",
       " 'no_context_prompt': 'Give me an insight. ',\n",
       " 'abstracts': [\"Current benchmarks like ``$\\\\textit{Needle-in-a-Haystack}$'' ($\\\\textit{NIAH}$), $\\\\textit{Ruler}$, and $\\\\textit{Needlebench}$ focus on models' ability to understand long-context input sequences but fail to capture a critical dimension: the generation of high-quality long-form text. Applications such as design proposals, technical documentation, and creative writing rely on coherent, instruction-following outputs over extended sequences—a challenge that existing benchmarks do not adequately address. To fill this gap, we introduce $\\\\textit{LongGenBench}$, a novel benchmark designed to rigorously evaluate large language models' (LLMs) ability to generate long text while adhering to complex instructions. Through tasks requiring specific events or constraints within generated text, $\\\\textit{LongGenBench}$ evaluates model performance across four distinct scenarios, three instruction types, and two generation-lengths (16K and 32K tokens). Our evaluation of ten state-of-the-art LLMs reveals that, despite strong results on $\\\\textit{Ruler}$, all models struggled with long text generation on $\\\\textit{LongGenBench}$, particularly as text length increased. This suggests that current LLMs are not yet equipped to meet the demands of real-world, long-form text generation. We open-source $\\\\textit{LongGenBench}$ to promote comprehensive evaluation and improvement in this critical area, with code and data available at ${anonymousurl}$.\",\n",
       "  'Large Multimodal Models (LMMs) have made significant strides in visual question-answering for single images. Recent advancements like long-context LMMs have allowed them to ingest larger, or even multiple, images. However, the ability to process a large number of visual tokens does not guarantee effective retrieval and reasoning for multi-image question answering (MIQA), especially in real-world applications like photo album searches or satellite imagery analysis. In this work, we first assess the limitations of current benchmarks for long-context LMMs. We address these limitations by introducing a new vision-centric, long-context benchmark, \"Visual Haystacks (VHs)\". We comprehensively evaluate both open-source and proprietary models on VHs, and demonstrate that these models struggle when reasoning across potentially unrelated images, perform poorly on cross-image reasoning, as well as exhibit biases based on the placement of key information within the context window. Towards a solution, we introduce MIRAGE (Multi-Image Retrieval Augmented Generation), an open-source, lightweight visual-RAG framework that processes up to 10k images on a single 40G A100 GPU—far surpassing the 1k-image limit of contemporary models. MIRAGE demonstrates up to 13% performance improvement over existing open-source LMMs on VHs, sets a new state-of-the-art on the RetVQA multi-image QA benchmark, and achieves competitive performance on single-image QA with state-of-the-art LMMs. Our dataset, model, and code are available at: https://visual-haystacks.github.io.'],\n",
       " 'forum_id_1': '3A71qNKWAS',\n",
       " 'forum_id_2': '9JCNPFL1f9',\n",
       " 'pair_id': '3A71qNKWAS_9JCNPFL1f9'}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds['train'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d5b9465a244943a2aa8eb7f7d0338c20",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=24):   0%|          | 0/99897 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c9b84777a8f4dd8b3b2ba9a2d136765",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=24):   0%|          | 0/100 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['paper1_prompt', 'paper2_prompt', 'no_context_prompt', 'abstracts', 'forum_id_1', 'forum_id_2', 'pair_id', 'data_source', 'prompt', 'ability', 'reward_model', 'extra_info'],\n",
       "    num_rows: 99897\n",
       "})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from typing import Dict, Any, Optional\n",
    "import os\n",
    "def make_map_fn(split: str):\n",
    "    \"\"\"Create a mapping function to process dataset examples.\n",
    "\n",
    "    Args:\n",
    "        split: Dataset split name ('train' or 'test')\n",
    "\n",
    "    Returns:\n",
    "        Function that processes individual dataset examples\n",
    "    \"\"\"\n",
    "    def process_fn(example: Dict[str, Any], idx: int) -> Optional[Dict[str, Any]]:\n",
    "        question = example.pop('joint_prompt')\n",
    "        answer = '' # empty string (dummy)\n",
    "\n",
    "        data = {\n",
    "            \"data_source\": \"\",\n",
    "            \"prompt\": [{\n",
    "                \"role\": \"user\",\n",
    "                \"content\": question\n",
    "            }],\n",
    "            \"ability\": \"insight\",\n",
    "            \"reward_model\": {\n",
    "                \"style\": \"rule\",\n",
    "                \"ground_truth\": answer\n",
    "            },\n",
    "            \"extra_info\": {\n",
    "                'joint_prompt': question,\n",
    "                'paper1_prompt': example['paper1_prompt'],\n",
    "                'paper2_prompt': example['paper2_prompt'],\n",
    "                'no_context_prompt': example['no_context_prompt'],\n",
    "                'abstracts': example['abstracts'],\n",
    "                'forum_id_1': example['forum_id_1'],\n",
    "                'forum_id_2': example['forum_id_2'],\n",
    "                'pair_id': example['pair_id'],\n",
    "                'split': split,\n",
    "                'index': idx\n",
    "            }\n",
    "        }\n",
    "        return data\n",
    "    return process_fn\n",
    "\n",
    "ds_train = ds['train'].map(function=make_map_fn('train'), with_indices=True, num_proc=os.cpu_count())\n",
    "ds_test = ds['test'].map(function=make_map_fn('test'), with_indices=True, num_proc=os.cpu_count())\n",
    "\n",
    "\n",
    "ds_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bdbe17d8f75a423b955a1925ed1f91ab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=24):   0%|          | 0/99897 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a52348e20573431ead1c560139754c3a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=24):   0%|          | 0/100 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained('Qwen/Qwen2.5-3B')\n",
    "\n",
    "def map_tok_length(example):\n",
    "    example['prompt_tok_length'] = len(tokenizer.encode(example['prompt'][0]['content']))\n",
    "    return example\n",
    "\n",
    "ds_train = ds_train.map(map_tok_length, num_proc=os.cpu_count())\n",
    "ds_test = ds_test.map(map_tok_length, num_proc=os.cpu_count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean:  1079.125078831196\n",
      "Median:  1073.0\n",
      "Std:  85.79160184024276\n",
      "Min:  786\n",
      "Max:  1612\n",
      "99th percentile:  1307.0\n",
      "95th percentile:  1230.0\n",
      "90th percentile:  1191.0\n"
     ]
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "from seaborn import histplot\n",
    "all_tok_lengths = ds_train['prompt_tok_length']\n",
    "histplot(all_tok_lengths)\n",
    "plt.show()\n",
    "\n",
    "import numpy as np\n",
    "print('Mean: ', np.mean(all_tok_lengths))\n",
    "print('Median: ', np.median(all_tok_lengths))\n",
    "print('Std: ', np.std(all_tok_lengths))\n",
    "print('Min: ', np.min(all_tok_lengths))\n",
    "print('Max: ', np.max(all_tok_lengths))\n",
    "print('99th percentile: ', np.percentile(all_tok_lengths, 99))\n",
    "print('95th percentile: ', np.percentile(all_tok_lengths, 95))\n",
    "print('90th percentile: ', np.percentile(all_tok_lengths, 90))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean:  1077.55\n",
      "Median:  1079.0\n",
      "Std:  84.86794153271305\n",
      "Min:  888\n",
      "Max:  1288\n",
      "99th percentile:  1252.3600000000001\n",
      "95th percentile:  1211.35\n",
      "90th percentile:  1193.1\n"
     ]
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "from seaborn import histplot\n",
    "all_tok_lengths = ds_test['prompt_tok_length']\n",
    "histplot(all_tok_lengths)\n",
    "plt.show()\n",
    "\n",
    "import numpy as np\n",
    "print('Mean: ', np.mean(all_tok_lengths))\n",
    "print('Median: ', np.median(all_tok_lengths))\n",
    "print('Std: ', np.std(all_tok_lengths))\n",
    "print('Min: ', np.min(all_tok_lengths))\n",
    "print('Max: ', np.max(all_tok_lengths))\n",
    "print('99th percentile: ', np.percentile(all_tok_lengths, 99))\n",
    "print('95th percentile: ', np.percentile(all_tok_lengths, 95))\n",
    "print('90th percentile: ', np.percentile(all_tok_lengths, 90))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'abstracts': [\"Current benchmarks like ``$\\\\textit{Needle-in-a-Haystack}$'' ($\\\\textit{NIAH}$), $\\\\textit{Ruler}$, and $\\\\textit{Needlebench}$ focus on models' ability to understand long-context input sequences but fail to capture a critical dimension: the generation of high-quality long-form text. Applications such as design proposals, technical documentation, and creative writing rely on coherent, instruction-following outputs over extended sequences—a challenge that existing benchmarks do not adequately address. To fill this gap, we introduce $\\\\textit{LongGenBench}$, a novel benchmark designed to rigorously evaluate large language models' (LLMs) ability to generate long text while adhering to complex instructions. Through tasks requiring specific events or constraints within generated text, $\\\\textit{LongGenBench}$ evaluates model performance across four distinct scenarios, three instruction types, and two generation-lengths (16K and 32K tokens). Our evaluation of ten state-of-the-art LLMs reveals that, despite strong results on $\\\\textit{Ruler}$, all models struggled with long text generation on $\\\\textit{LongGenBench}$, particularly as text length increased. This suggests that current LLMs are not yet equipped to meet the demands of real-world, long-form text generation. We open-source $\\\\textit{LongGenBench}$ to promote comprehensive evaluation and improvement in this critical area, with code and data available at ${anonymousurl}$.\",\n",
       "  'Large Multimodal Models (LMMs) have made significant strides in visual question-answering for single images. Recent advancements like long-context LMMs have allowed them to ingest larger, or even multiple, images. However, the ability to process a large number of visual tokens does not guarantee effective retrieval and reasoning for multi-image question answering (MIQA), especially in real-world applications like photo album searches or satellite imagery analysis. In this work, we first assess the limitations of current benchmarks for long-context LMMs. We address these limitations by introducing a new vision-centric, long-context benchmark, \"Visual Haystacks (VHs)\". We comprehensively evaluate both open-source and proprietary models on VHs, and demonstrate that these models struggle when reasoning across potentially unrelated images, perform poorly on cross-image reasoning, as well as exhibit biases based on the placement of key information within the context window. Towards a solution, we introduce MIRAGE (Multi-Image Retrieval Augmented Generation), an open-source, lightweight visual-RAG framework that processes up to 10k images on a single 40G A100 GPU—far surpassing the 1k-image limit of contemporary models. MIRAGE demonstrates up to 13% performance improvement over existing open-source LMMs on VHs, sets a new state-of-the-art on the RetVQA multi-image QA benchmark, and achieves competitive performance on single-image QA with state-of-the-art LMMs. Our dataset, model, and code are available at: https://visual-haystacks.github.io.'],\n",
       " 'forum_id_1': '3A71qNKWAS',\n",
       " 'forum_id_2': '9JCNPFL1f9',\n",
       " 'index': 0,\n",
       " 'joint_prompt': 'Your task is to identify and elaborate on an insight that only becomes apparent by combining information from both documents together—i.e., an insight that has high relevance when treating the documents jointly but low relevance if you were to consider each document alone. Write the insight as a standalone statement that does not reference the original texts or use terms like \"paper\" or \"document\". The insight should involve indirect connections or intermediate reasoning steps, such as linking cause and effect through a shared variable or mechanism. Here are some examples: Let A = magnesium — either dietary or internal to the body. Implicitly refers, inmost cases, to quantity of magnesium. Let M = migraine headache. Let L = one or more intermediate physiological links.  -> means \\'can or might influence\\'. == means \\'equivalent in action\\' or \\'equivalent in mechanism\\' depending on whether it connects two drugs or two diseases. Paper 1:\\nStress and Type A behavior are associated with migraine.\\nPaper 2:\\nStress and Type A behavior lead to body loss of magnesium.\\nFrom these, we infer: Let B = Stress and Type A behavior. Let A = Magnesium levels. Let M = Migraines. From paper 1: B -> M. From Paper 2: B -> A. This suggests A may mediate the effect of B on M, i.e., A -> M. <insight>Low magnesium levels may be an intermediate link through which stress and Type A behavior contribute to migraines.</insight>\\nPaper 1:\\nExcessive vascular tone and reactivity may increase susceptibility to migraine.\\nPaper 2:\\nMagnesium can reduce vascular tone and reactivity.\\nWe define: A = Magnesium. L = Vascular tone and reactivity. M = Migraine. From these: (a) suggests L -> M. (b) suggests A -> L. Therefore, combining the two leads to: A -> M. <insight>Magnesium may help reduce migraine susceptibility by lowering vascular tone and reactivity.</insight>\\nPaper 1:\\nCalcium channel blockers have been used successfully in preventing migraine attacks.\\nPaper 2:\\nMagnesium is a natural calcium channel blocker.\\nWe define: A = Magnesium. B = Calcium channel blockers. M = Migraine. From these:(a) suggests B -> M (i.e., B prevents M). (b) suggests A == B (A is equivalent to B in mechanism). Therefore: A -> M. <insight>Magnesium may help prevent migraine attacks because it acts as a natural calcium channel blocker, similar to drugs already known to be effective.</insight>\\nPaper 1:\\nCurrent benchmarks like ``$\\\\textit{Needle-in-a-Haystack}$\\'\\' ($\\\\textit{NIAH}$), $\\\\textit{Ruler}$, and $\\\\textit{Needlebench}$ focus on models\\' ability to understand long-context input sequences but fail to capture a critical dimension: the generation of high-quality long-form text. Applications such as design proposals, technical documentation, and creative writing rely on coherent, instruction-following outputs over extended sequences—a challenge that existing benchmarks do not adequately address. To fill this gap, we introduce $\\\\textit{LongGenBench}$, a novel benchmark designed to rigorously evaluate large language models\\' (LLMs) ability to generate long text while adhering to complex instructions. Through tasks requiring specific events or constraints within generated text, $\\\\textit{LongGenBench}$ evaluates model performance across four distinct scenarios, three instruction types, and two generation-lengths (16K and 32K tokens). Our evaluation of ten state-of-the-art LLMs reveals that, despite strong results on $\\\\textit{Ruler}$, all models struggled with long text generation on $\\\\textit{LongGenBench}$, particularly as text length increased. This suggests that current LLMs are not yet equipped to meet the demands of real-world, long-form text generation. We open-source $\\\\textit{LongGenBench}$ to promote comprehensive evaluation and improvement in this critical area, with code and data available at ${anonymousurl}$.\\nPaper 2:\\nLarge Multimodal Models (LMMs) have made significant strides in visual question-answering for single images. Recent advancements like long-context LMMs have allowed them to ingest larger, or even multiple, images. However, the ability to process a large number of visual tokens does not guarantee effective retrieval and reasoning for multi-image question answering (MIQA), especially in real-world applications like photo album searches or satellite imagery analysis. In this work, we first assess the limitations of current benchmarks for long-context LMMs. We address these limitations by introducing a new vision-centric, long-context benchmark, \"Visual Haystacks (VHs)\". We comprehensively evaluate both open-source and proprietary models on VHs, and demonstrate that these models struggle when reasoning across potentially unrelated images, perform poorly on cross-image reasoning, as well as exhibit biases based on the placement of key information within the context window. Towards a solution, we introduce MIRAGE (Multi-Image Retrieval Augmented Generation), an open-source, lightweight visual-RAG framework that processes up to 10k images on a single 40G A100 GPU—far surpassing the 1k-image limit of contemporary models. MIRAGE demonstrates up to 13% performance improvement over existing open-source LMMs on VHs, sets a new state-of-the-art on the RetVQA multi-image QA benchmark, and achieves competitive performance on single-image QA with state-of-the-art LMMs. Our dataset, model, and code are available at: https://visual-haystacks.github.io.\\nPut the insight between <insight> and </insight> tagsLet\\'s think step by step. ',\n",
       " 'no_context_prompt': 'Give me an insight. ',\n",
       " 'pair_id': '3A71qNKWAS_9JCNPFL1f9',\n",
       " 'paper1_prompt': 'Paper:\\nCurrent benchmarks like ``$\\\\textit{Needle-in-a-Haystack}$\\'\\' ($\\\\textit{NIAH}$), $\\\\textit{Ruler}$, and $\\\\textit{Needlebench}$ focus on models\\' ability to understand long-context input sequences but fail to capture a critical dimension: the generation of high-quality long-form text. Applications such as design proposals, technical documentation, and creative writing rely on coherent, instruction-following outputs over extended sequences—a challenge that existing benchmarks do not adequately address. To fill this gap, we introduce $\\\\textit{LongGenBench}$, a novel benchmark designed to rigorously evaluate large language models\\' (LLMs) ability to generate long text while adhering to complex instructions. Through tasks requiring specific events or constraints within generated text, $\\\\textit{LongGenBench}$ evaluates model performance across four distinct scenarios, three instruction types, and two generation-lengths (16K and 32K tokens). Our evaluation of ten state-of-the-art LLMs reveals that, despite strong results on $\\\\textit{Ruler}$, all models struggled with long text generation on $\\\\textit{LongGenBench}$, particularly as text length increased. This suggests that current LLMs are not yet equipped to meet the demands of real-world, long-form text generation. We open-source $\\\\textit{LongGenBench}$ to promote comprehensive evaluation and improvement in this critical area, with code and data available at ${anonymousurl}$.\\nYour task is to identify and elaborate on an insight from the paper. The insight should be self-contained. Write it in a way that doesn’t require referencing where it came from. Do not mention the word \"paper\" or \"document\". Let\\'s think step by step. ',\n",
       " 'paper2_prompt': 'Paper:\\nLarge Multimodal Models (LMMs) have made significant strides in visual question-answering for single images. Recent advancements like long-context LMMs have allowed them to ingest larger, or even multiple, images. However, the ability to process a large number of visual tokens does not guarantee effective retrieval and reasoning for multi-image question answering (MIQA), especially in real-world applications like photo album searches or satellite imagery analysis. In this work, we first assess the limitations of current benchmarks for long-context LMMs. We address these limitations by introducing a new vision-centric, long-context benchmark, \"Visual Haystacks (VHs)\". We comprehensively evaluate both open-source and proprietary models on VHs, and demonstrate that these models struggle when reasoning across potentially unrelated images, perform poorly on cross-image reasoning, as well as exhibit biases based on the placement of key information within the context window. Towards a solution, we introduce MIRAGE (Multi-Image Retrieval Augmented Generation), an open-source, lightweight visual-RAG framework that processes up to 10k images on a single 40G A100 GPU—far surpassing the 1k-image limit of contemporary models. MIRAGE demonstrates up to 13% performance improvement over existing open-source LMMs on VHs, sets a new state-of-the-art on the RetVQA multi-image QA benchmark, and achieves competitive performance on single-image QA with state-of-the-art LMMs. Our dataset, model, and code are available at: https://visual-haystacks.github.io.\\nYour task is to identify and elaborate on an insight from the paper. The insight should be self-contained. Write it in a way that doesn’t require referencing where it came from. Do not mention the word \"paper\" or \"document\". Let\\'s think step by step. ',\n",
       " 'split': 'train'}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds_train['extra_info'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f77cfe25c86f4dd4a5c32df762188598",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/100 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "12e052955eb54130b261a1f12bb4b58a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "2281384"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds_train.to_parquet('/home/anikait.singh/rl_behaviors_verl_stable/data_insights_rl/train.parquet')\n",
    "ds_test.to_parquet('/home/anikait.singh/rl_behaviors_verl_stable/data_insights_rl/test.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['paper1_prompt', 'paper2_prompt', 'no_context_prompt', 'abstracts', 'forum_id_1', 'forum_id_2', 'pair_id', 'data_source', 'prompt', 'ability', 'reward_model', 'extra_info', 'prompt_tok_length'],\n",
       "    num_rows: 99897\n",
       "})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['paper1_prompt', 'paper2_prompt', 'no_context_prompt', 'abstracts', 'forum_id_1', 'forum_id_2', 'pair_id', 'data_source', 'prompt', 'ability', 'reward_model', 'extra_info', 'prompt_tok_length'],\n",
       "    num_rows: 100\n",
       "})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e45ad0b16f0d4793bd1f005f65c8e0c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter (num_proc=24):   0%|          | 0/100 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['paper1_prompt', 'paper2_prompt', 'no_context_prompt', 'abstracts', 'forum_id_1', 'forum_id_2', 'pair_id', 'data_source', 'prompt', 'ability', 'reward_model', 'extra_info', 'prompt_tok_length'],\n",
       "    num_rows: 99\n",
       "})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds_test_filt = ds_test.filter(lambda x: x['prompt_tok_length'] < 1280, num_proc=os.cpu_count())\n",
    "ds_test_filt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6df81d7ff0fe4d599617263547733ea9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading the dataset shards:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fd6b95c970cd42bba7065ee395752bcf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/20 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "38a03d4690c5403b9ed216516b205f96",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/20 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "720e3627a31e45e5b5038dc18a65a5fc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/20 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "534e2738418646d7b814bee9faf4d9ab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/20 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5fa0c8dcefa84323ab646ade1a60c554",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/20 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bca7b20f07994f62b5dcafa84dfc6380",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading the dataset shards:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "943c357afca1405980c7c4f5f62d534d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/datasets/Asap7772/insight_rl/commit/b5b10268529cad659e396a6a0224ac354457392d', commit_message='Upload dataset', commit_description='', oid='b5b10268529cad659e396a6a0224ac354457392d', pr_url=None, repo_url=RepoUrl('https://huggingface.co/datasets/Asap7772/insight_rl', endpoint='https://huggingface.co', repo_type='dataset', repo_id='Asap7772/insight_rl'), pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds_dict = datasets.DatasetDict({\n",
    "    'train': ds_train,\n",
    "    'test': ds_test\n",
    "})\n",
    "ds_dict.push_to_hub('Asap7772/insight_rl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your task is to identify and elaborate on an insight that only becomes apparent by combining information from both documents together—i.e., an insight that has high relevance when treating the documents jointly but low relevance if you were to consider each document alone. Write the insight as a standalone statement that does not reference the original texts or use terms like \"paper\" or \"document\". The insight should involve indirect connections or intermediate reasoning steps, such as linking cause and effect through a shared variable or mechanism. Here are some examples: Let A = magnesium — either dietary or internal to the body. Implicitly refers, inmost cases, to quantity of magnesium. Let M = migraine headache. Let L = one or more intermediate physiological links.  -> means 'can or might influence'. == means 'equivalent in action' or 'equivalent in mechanism' depending on whether it connects two drugs or two diseases. Paper 1:\n",
      "Stress and Type A behavior are associated with migraine.\n",
      "Paper 2:\n",
      "Stress and Type A behavior lead to body loss of magnesium.\n",
      "From these, we infer: Let B = Stress and Type A behavior. Let A = Magnesium levels. Let M = Migraines. From paper 1: B -> M. From Paper 2: B -> A. This suggests A may mediate the effect of B on M, i.e., A -> M. <insight>Low magnesium levels may be an intermediate link through which stress and Type A behavior contribute to migraines.</insight>\n",
      "Paper 1:\n",
      "Excessive vascular tone and reactivity may increase susceptibility to migraine.\n",
      "Paper 2:\n",
      "Magnesium can reduce vascular tone and reactivity.\n",
      "We define: A = Magnesium. L = Vascular tone and reactivity. M = Migraine. From these: (a) suggests L -> M. (b) suggests A -> L. Therefore, combining the two leads to: A -> M. <insight>Magnesium may help reduce migraine susceptibility by lowering vascular tone and reactivity.</insight>\n",
      "Paper 1:\n",
      "Calcium channel blockers have been used successfully in preventing migraine attacks.\n",
      "Paper 2:\n",
      "Magnesium is a natural calcium channel blocker.\n",
      "We define: A = Magnesium. B = Calcium channel blockers. M = Migraine. From these:(a) suggests B -> M (i.e., B prevents M). (b) suggests A == B (A is equivalent to B in mechanism). Therefore: A -> M. <insight>Magnesium may help prevent migraine attacks because it acts as a natural calcium channel blocker, similar to drugs already known to be effective.</insight>\n",
      "Paper 1:\n",
      "Current benchmarks like ``$\\textit{Needle-in-a-Haystack}$'' ($\\textit{NIAH}$), $\\textit{Ruler}$, and $\\textit{Needlebench}$ focus on models' ability to understand long-context input sequences but fail to capture a critical dimension: the generation of high-quality long-form text. Applications such as design proposals, technical documentation, and creative writing rely on coherent, instruction-following outputs over extended sequences—a challenge that existing benchmarks do not adequately address. To fill this gap, we introduce $\\textit{LongGenBench}$, a novel benchmark designed to rigorously evaluate large language models' (LLMs) ability to generate long text while adhering to complex instructions. Through tasks requiring specific events or constraints within generated text, $\\textit{LongGenBench}$ evaluates model performance across four distinct scenarios, three instruction types, and two generation-lengths (16K and 32K tokens). Our evaluation of ten state-of-the-art LLMs reveals that, despite strong results on $\\textit{Ruler}$, all models struggled with long text generation on $\\textit{LongGenBench}$, particularly as text length increased. This suggests that current LLMs are not yet equipped to meet the demands of real-world, long-form text generation. We open-source $\\textit{LongGenBench}$ to promote comprehensive evaluation and improvement in this critical area, with code and data available at ${anonymousurl}$.\n",
      "Paper 2:\n",
      "Large Multimodal Models (LMMs) have made significant strides in visual question-answering for single images. Recent advancements like long-context LMMs have allowed them to ingest larger, or even multiple, images. However, the ability to process a large number of visual tokens does not guarantee effective retrieval and reasoning for multi-image question answering (MIQA), especially in real-world applications like photo album searches or satellite imagery analysis. In this work, we first assess the limitations of current benchmarks for long-context LMMs. We address these limitations by introducing a new vision-centric, long-context benchmark, \"Visual Haystacks (VHs)\". We comprehensively evaluate both open-source and proprietary models on VHs, and demonstrate that these models struggle when reasoning across potentially unrelated images, perform poorly on cross-image reasoning, as well as exhibit biases based on the placement of key information within the context window. Towards a solution, we introduce MIRAGE (Multi-Image Retrieval Augmented Generation), an open-source, lightweight visual-RAG framework that processes up to 10k images on a single 40G A100 GPU—far surpassing the 1k-image limit of contemporary models. MIRAGE demonstrates up to 13% performance improvement over existing open-source LMMs on VHs, sets a new state-of-the-art on the RetVQA multi-image QA benchmark, and achieves competitive performance on single-image QA with state-of-the-art LMMs. Our dataset, model, and code are available at: https://visual-haystacks.github.io.\n",
      "Put the insight between <insight> and </insight> tagsLet's think step by step. \n"
     ]
    }
   ],
   "source": [
    "print(ds_train[0]['extra_info']['joint_prompt'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "verl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
